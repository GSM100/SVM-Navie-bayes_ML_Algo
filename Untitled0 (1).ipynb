{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/YZkG4JsHMKsyUMfSNyMy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Question 1: What is a Support Vector Machine (SVM), and how does it work?\n","\n","-  Support Vector Machine (SVM)\n","\n","A Support Vector Machine is a supervised machine learning algorithm used for classification (mainly) and regression tasks.\n","\n","It works by finding the optimal decision boundary (hyperplane) that separates different classes in the feature space.\n","\n","How It Works (Step by Step)\n","1. Decision Boundary (Hyperplane)\n","\n","- In 2D space, the decision boundary is a line.\n","\n","- In 3D space, it‚Äôs a plane.\n","\n","- In higher dimensions, it‚Äôs called a hyperplane.\n","\n","- The goal is to find the hyperplane that maximizes the margin (distance between the hyperplane and the nearest data points).\n","\n","2. Support Vectors\n","\n","- The data points closest to the hyperplane are called support vectors.\n","\n","- They are critical, because if you remove them, the position of the hyperplane changes.\n","\n","- Only these points determine the boundary, not the others.\n","\n","3. Margin Maximization\n","\n","- SVM tries to maximize the margin = distance between the hyperplane and the nearest points of each class.\n","\n","- A larger margin means better generalization and lower risk of overfitting.\n","\n","4. Handling Non-Linearly Separable Data\n","\n","- If the data is not linearly separable, SVM uses the Kernel Trick.\n","\n","- Kernel functions (like Polynomial, RBF/Gaussian, Sigmoid) map data into higher-dimensional space where a linear separation is possible.\n","\n","5. Soft Margin (Handling Noise & Outliers)\n","\n","- Real-world data is often noisy.\n","\n","- SVM introduces a parameter C (regularization):\n","\n","- Large C ‚Üí strict classification, less tolerance for misclassification.\n","\n","- Small C ‚Üí allows some misclassification, better generalization."],"metadata":{"id":"_M-hNHcGLxrM"}},{"cell_type":"markdown","source":["Question 2: Explain the difference between Hard Margin and Soft Margin SVM\n","\n","-  Hard Margin SVM\n","\n","- Assumes the dataset is perfectly linearly separable (no overlap, no noise).\n","\n","- The SVM finds a hyperplane that separates the classes without any misclassification.\n","\n","- The margin is maximized and all data points must lie outside or on the margin boundaries.\n","\n","üîπ Advantages:\n","\n","- Simple and works well if data is perfectly clean and separable.\n","\n","üîπ Disadvantages:\n","\n","- Very sensitive to outliers and noise.\n","\n","- If just one point is misclassified, the hard margin solution may fail.\n","\n","Soft Margin SVM :-\n","\n","- Used when data is not perfectly separable (real-world cases).\n","\n","- Introduces a slack variable (Œæ) that allows some misclassifications.\n","\n","- Adds a regularization parameter (C) to control trade-off:\n","\n","- Large C ‚Üí fewer misclassifications (harder margin, risk of overfitting).\n","\n","- Small C ‚Üí more tolerance to misclassifications (softer margin, better generalization).\n","\n","üîπ Advantages:\n","\n","- Robust to noise and outliers.\n","\n","- Works well in practical datasets that are not perfectly separable.\n","\n","üîπ Disadvantages:\n","\n","- Needs proper tuning of C (regularization)."],"metadata":{"id":"gSMoUXUqMr0Q"}},{"cell_type":"markdown","source":["Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n","explain its use case.\n","\n","-  Kernel Trick in SVM\n","\n","Many datasets are not linearly separable in their original feature space.\n","\n","The kernel trick allows SVM to implicitly map data into a higher-dimensional space where it becomes linearly separable.\n","\n","Importantly, the kernel trick avoids explicitly computing the high-dimensional transformation (which could be computationally expensive).\n","\n","Instead, it uses a kernel function that directly computes the dot product of two vectors in the higher-dimensional feature space.\n","\n","üëâ This way, SVM can create non-linear decision boundaries while still solving the optimization problem efficiently.\n","\n","Example: Radial Basis Function (RBF) Kernel\n","\n","Formula:\n","\n","ùêæ\n","(\n","ùë•\n",",\n","ùë•\n","‚Ä≤\n",")\n","= exp\n","‚Å°\n","(\n","‚àí\n","ùõæ\n","‚à•\n","ùë•\n","‚àí\n","ùë•\n","‚Ä≤\n","‚à•\n","^2\n",")\n","\n","where\n","\n","ùë•\n",",\n","ùë•\n","‚Ä≤\n"," = data points\n","\n","ùõæ\n","= controls the influence of a single training example\n","Use Case\n","\n","Suppose you have a dataset shaped like concentric circles (inner circle = class 0, outer circle = class 1).\n","\n","In 2D space, this is not linearly separable.\n","\n","With an RBF kernel, SVM maps the data into higher dimensions where a simple linear separation (hyperplane) is possible."],"metadata":{"id":"RxXGDqlxNVvW"}},{"cell_type":"markdown","source":["Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n","\n","-  Na√Øve Bayes Classifier\n","\n","- A probabilistic supervised learning algorithm based on Bayes‚Äô Theorem.\n","\n","Commonly used for classification tasks such as spam filtering, sentiment analysis, and text classification.\n","\n","- It predicts the class of a sample based on the posterior probability:\n","\n","  - P(Class‚à£Features) = P(Features‚à£Class)‚ãÖP(Class)‚Äã/(Features)\n","‚Äã\n","\n","- Why is it called ‚ÄúNa√Øve‚Äù?\n","\n","- Because it makes the na√Øve assumption that all features are independent of each other given the class label.\n","\n","- In reality, features often correlate (e.g., in text classification, the words ‚Äúfree‚Äù and ‚Äúoffer‚Äù are not independent).\n","\n","- Despite this unrealistic assumption, Na√Øve Bayes works surprisingly well in many domains, especially with high-dimensional data (like text).\n","\n","- Key Variants\n","\n","- Gaussian Na√Øve Bayes ‚Üí assumes features follow a normal distribution (e.g., continuous data like age, income).\n","\n","- Multinomial Na√Øve Bayes ‚Üí used for discrete count features (e.g., word counts in text classification).\n","\n","- Bernoulli Na√Øve Bayes ‚Üí used for binary features (e.g., word present or absent in a document).\n","\n","Advantages\n","\n","- Very fast and efficient for large datasets.\n","\n","- Works well with text classification and spam detection.\n","\n","- Requires less training data.\n","\n","Disadvantages\n","\n","- Independence assumption rarely holds in real-world data.\n","\n","- Performs poorly when features are highly correlated."],"metadata":{"id":"ExuXGpEyOHSE"}},{"cell_type":"markdown","source":["Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants.\n","When would you use each one?\n","\n"," - 1. Gaussian Na√Øve Bayes\n","\n","  - Assumes that the features follow a normal (Gaussian) distribution.\n","\n","  - Used for continuous data.\n","\n","  - For each class, it estimates the mean and variance of the feature values, then applies Bayes‚Äô theorem.\n","\n","  - Predicting whether a patient has a disease based on continuous features (age, blood pressure, cholesterol).\n","\n","- Iris flower classification (sepal length, petal width, etc.).\n","\n","2. Multinomial Na√Øve Bayes\n","\n","- Assumes features are discrete counts (like word frequencies).\n","\n","- The likelihood is based on multinomial distribution.\n","\n","- Very popular in text classification tasks.\n","\n"," - Use case examples:\n","\n","- Spam filtering (count of words like ‚Äúfree‚Äù, ‚Äúoffer‚Äù).\n","\n","- Sentiment analysis (word frequency in reviews).\n","\n","- Document categorization (news, sports, politics).\n","\n","3. Bernoulli Na√Øve Bayes\n","\n","- Assumes features are binary (0 or 1) ‚Üí whether a feature is present or absent.\n","\n","- Instead of using counts, it only considers presence/absence of features.\n","\n","- Use case examples:\n","\n","- Text classification where features are ‚Äúword present (1)‚Äù or ‚Äúword absent (0)‚Äù.\n","\n","- Sentiment classification with binary indicators for specific keywords.\n","\n","- Simple recommendation systems (user has clicked/not clicked on an item)."],"metadata":{"id":"pyXIhe7nPykC"}},{"cell_type":"code","source":["# Import libraries\n","from sklearn.datasets import load_iris, load_breast_cancer, fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n","from sklearn.preprocessing import Binarizer\n","from sklearn.metrics import accuracy_score\n","\n","# -------------------------------\n","# 1. Gaussian Na√Øve Bayes on Iris dataset\n","# -------------------------------\n","iris = load_iris()\n","X_train, X_test, y_train, y_test = train_test_split(\n","    iris.data, iris.target, test_size=0.2, random_state=42\n",")\n","\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","y_pred = gnb.predict(X_test)\n","print(\"GaussianNB (Iris dataset) Accuracy:\", accuracy_score(y_test, y_pred))\n","\n","# -------------------------------\n","# 2. Bernoulli Na√Øve Bayes on Breast Cancer dataset\n","# -------------------------------\n","cancer = load_breast_cancer()\n","# Binarize continuous features (turn into 0/1 for BernoulliNB)\n","binarizer = Binarizer(threshold=cancer.data.mean())\n","X_binarized = binarizer.fit_transform(cancer.data)\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_binarized, cancer.target, test_size=0.2, random_state=42\n",")\n","\n","bnb = BernoulliNB()\n","bnb.fit(X_train, y_train)\n","y_pred = bnb.predict(X_test)\n","print(\"BernoulliNB (Breast Cancer dataset) Accuracy:\", accuracy_score(y_test, y_pred))\n","\n","# -------------------------------\n","# 3. Multinomial Na√Øve Bayes on Text Data (20 Newsgroups)\n","# -------------------------------\n","categories = ['alt.atheism', 'sci.space']  # just 2 categories for speed\n","newsgroups = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n","\n","# Convert text to word counts\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(newsgroups.data)\n","y = newsgroups.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","mnb = MultinomialNB()\n","mnb.fit(X_train, y_train)\n","y_pred = mnb.predict(X_test)\n","print(\"MultinomialNB (Text dataset) Accuracy:\", accuracy_score(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f17gBLKCRAjh","executionInfo":{"status":"ok","timestamp":1758122511849,"user_tz":-330,"elapsed":18052,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"ad62c4d1-7faa-4be0-a2a3-9905c2c7ff11"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["GaussianNB (Iris dataset) Accuracy: 1.0\n","BernoulliNB (Breast Cancer dataset) Accuracy: 0.8070175438596491\n","MultinomialNB (Text dataset) Accuracy: 0.9953488372093023\n"]}]},{"cell_type":"markdown","source":["Question 6: Write a Python program to: ‚óè Load the Iris dataset ‚óè Train an SVM Classifier with a linear kernel ‚óè Print the model's accuracy and support vectors."],"metadata":{"id":"cS29S8CNRHf8"}},{"cell_type":"code","source":["# Import libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Initialize SVM with linear kernel\n","svm_clf = SVC(kernel='linear', random_state=42)\n","\n","# Train the model\n","svm_clf.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = svm_clf.predict(X_test)\n","\n","# Accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"SVM Classifier Accuracy:\", accuracy)\n","\n","# Support vectors\n","print(\"Support Vectors:\\n\", svm_clf.support_vectors_)\n","print(\"Number of support vectors per class:\", svm_clf.n_support_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOsNSRTaRIh9","executionInfo":{"status":"ok","timestamp":1758122526575,"user_tz":-330,"elapsed":1333,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"63763e9d-065f-4f7b-9ce6-019f325d67ab"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Classifier Accuracy: 1.0\n","Support Vectors:\n"," [[4.8 3.4 1.9 0.2]\n"," [5.1 3.3 1.7 0.5]\n"," [4.5 2.3 1.3 0.3]\n"," [5.6 3.  4.5 1.5]\n"," [5.4 3.  4.5 1.5]\n"," [6.7 3.  5.  1.7]\n"," [5.9 3.2 4.8 1.8]\n"," [5.1 2.5 3.  1.1]\n"," [6.  2.7 5.1 1.6]\n"," [6.3 2.5 4.9 1.5]\n"," [6.1 2.9 4.7 1.4]\n"," [6.5 2.8 4.6 1.5]\n"," [6.9 3.1 4.9 1.5]\n"," [6.3 2.3 4.4 1.3]\n"," [6.3 2.5 5.  1.9]\n"," [6.3 2.8 5.1 1.5]\n"," [6.3 2.7 4.9 1.8]\n"," [6.  3.  4.8 1.8]\n"," [6.  2.2 5.  1.5]\n"," [6.2 2.8 4.8 1.8]\n"," [6.5 3.  5.2 2. ]\n"," [7.2 3.  5.8 1.6]\n"," [5.6 2.8 4.9 2. ]\n"," [5.9 3.  5.1 1.8]\n"," [4.9 2.5 4.5 1.7]]\n","Number of support vectors per class: [ 3 11 11]\n"]}]},{"cell_type":"markdown","source":["Question 7: Write a Python program to: ‚óè Load the Breast Cancer dataset ‚óè Train a Gaussian Na√Øve Bayes model ‚óè Print its classification report including precision, recall, and F1-score"],"metadata":{"id":"yYG56b6XRQ41"}},{"cell_type":"code","source":["# Import libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import classification_report\n","\n","# Load Breast Cancer dataset\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Initialize Gaussian Na√Øve Bayes\n","gnb = GaussianNB()\n","\n","# Train the model\n","gnb.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = gnb.predict(X_test)\n","\n","# Classification report\n","print(\"Gaussian Na√Øve Bayes - Breast Cancer Dataset\")\n","print(classification_report(y_test, y_pred, target_names=data.target_names))\n"],"metadata":{"id":"2xbgwz9KRR-p","executionInfo":{"status":"ok","timestamp":1758122566037,"user_tz":-330,"elapsed":8,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"f17ce385-ad72-46db-cc3c-67ba45168f6d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Gaussian Na√Øve Bayes - Breast Cancer Dataset\n","              precision    recall  f1-score   support\n","\n","   malignant       1.00      0.93      0.96        43\n","      benign       0.96      1.00      0.98        71\n","\n","    accuracy                           0.97       114\n","   macro avg       0.98      0.97      0.97       114\n","weighted avg       0.97      0.97      0.97       114\n","\n"]}]}]}